{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs  \n",
    "import urllib.request  \n",
    "import re  \n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\des.aaahli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\des.aaahli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we download the article using the urlopen method from the request class and urllib library\n",
    "scrapped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "#reading the data that we got from the website\n",
    "article = scrapped_data.read()\n",
    "#parse the data = transform the data into objects or variables using beatiful soup library\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "#Wikipedia stores the text content of the article inside p tags. We use the find_all function of the \n",
    "#BeautifulSoup object to fetch all the contents from the paragraph tags of the article.\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "#Finally, we join all the paragraphs together and store the scraped article in article_text variable for later use\n",
    "article_text = \"\"\n",
    "for p in paragraphs:\n",
    "    article_text += p.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data or text (Cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning text\n",
    "#-- Transform all text to lowercase\n",
    "processed_article = article_text.lower()\n",
    "# -- Remove all digits, special characters, or spaces\n",
    "processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )  \n",
    "processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "\n",
    "# Now we will get all sentences by\n",
    "#tokenizing our article into sentences. To do that we use sent_tokenize method from nltk library\n",
    "all_sentences = nltk.sent_tokenize(processed_article)\n",
    "\n",
    "# After getting all sentences we get all the words from our sentences. To do that we use word_tokenize method from nltk library\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "# removing stop words\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating our Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With Gensim, it is extremely straightforward to create Word2Vec model. \n",
    "#The word list is passed to the Word2Vec class of the gensim.models package.\n",
    "# We need to specify the value for the min_count parameter. \n",
    "#A value of 2 for min_count specifies to include only those words in the Word2Vec\n",
    "#model that appear at least twice in the corpus.\n",
    "word2vec = Word2Vec(all_words,min_count = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To see the dictionary of unique words that exist at least twice in the corpus\n",
    "vocabulary = word2vec.wv.vocab  \n",
    "# print(vocabulary)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the vector of 1 word\n",
    "v1 = word2vec.wv['computer']\n",
    "# Finding Similarity of 2 words\n",
    "sim_words = word2vec.wv.most_similar('human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.4857474e-04, -4.6664299e-03, -6.4516980e-03,  5.4080859e-03,\n",
       "       -2.7565183e-03, -2.6748810e-05,  1.3596782e-03, -8.3845417e-04,\n",
       "       -6.7246468e-03,  6.6225477e-03,  2.8466836e-03,  1.1640706e-03,\n",
       "        8.7220650e-03,  3.2808762e-03, -4.8386017e-03,  1.4625752e-03,\n",
       "       -4.1655512e-03,  2.6452395e-03, -3.4726453e-03,  6.8333261e-03,\n",
       "        4.0410315e-03, -4.7087595e-03,  6.5313810e-03, -4.1944566e-03,\n",
       "        2.1337003e-03, -3.6721458e-03, -9.0739824e-04,  1.4999218e-03,\n",
       "        2.8383299e-03, -2.8894925e-03, -4.1190516e-03, -1.8095972e-03,\n",
       "        3.4063691e-03,  2.0936815e-04,  3.4970280e-03,  3.8302769e-03,\n",
       "       -5.7361607e-04, -2.1917874e-03,  1.5966026e-03,  1.5429519e-03,\n",
       "       -6.0311812e-03,  2.6316603e-03,  3.1924290e-03, -2.8653883e-03,\n",
       "       -2.4439844e-03,  4.0175244e-03,  3.0204421e-04, -1.3016707e-03,\n",
       "       -4.5125391e-03, -5.8100699e-03, -1.4248654e-03,  2.5602682e-03,\n",
       "       -2.4976984e-03, -9.2291262e-04, -3.7116765e-03,  1.9296797e-03,\n",
       "        5.7843574e-03, -3.2131134e-03, -4.1826437e-03, -3.3104608e-03,\n",
       "       -3.0229492e-03, -6.2363804e-04,  1.6120627e-03,  9.5313992e-03,\n",
       "        4.1496917e-03,  1.0166892e-03, -4.7970163e-03, -8.0548640e-04,\n",
       "        1.7156032e-03,  9.9650677e-03,  3.7864053e-03, -1.2692168e-03,\n",
       "        6.2906637e-04, -6.5358025e-03, -1.0854094e-02, -7.1851141e-03,\n",
       "       -7.4165338e-03,  3.4658585e-04, -5.5801508e-04, -1.0018582e-03,\n",
       "       -3.4708114e-04, -3.2766452e-03,  2.1922982e-03, -9.1109722e-04,\n",
       "        2.5275892e-03, -1.6592105e-04,  3.4699414e-04, -9.7450038e-04,\n",
       "       -2.3759785e-03, -1.9516818e-03,  2.0257758e-03, -3.3991619e-03,\n",
       "        1.7467657e-03, -1.2731146e-03, -5.6234226e-03,  3.3172844e-03,\n",
       "       -2.8670421e-03,  5.3575444e-03,  4.2534764e-03,  5.3006760e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ai', 0.6996504068374634),\n",
       " ('artificial', 0.6545079946517944),\n",
       " ('search', 0.6144258379936218),\n",
       " ('intelligence', 0.6116807460784912),\n",
       " ('machines', 0.5917956829071045),\n",
       " ('networks', 0.5759385228157043),\n",
       " ('ethics', 0.573891818523407),\n",
       " ('neural', 0.5702526569366455),\n",
       " ('researchers', 0.5695855021476746),\n",
       " ('algorithms', 0.5653101205825806)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.dumps(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
